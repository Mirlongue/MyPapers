## Deep Unsupervised Part-Whole Relational Visual Saliency

### abstract

 Deep Supervised Salient Object Detection (SSOD) excessively relies on large-scale annotated pixel level labels which consume intensive labour acquiring high quality labels. In such precondition, deep Unsupervised Salient Object Detection (USOD) draws public attention. Under the framework of the existing deep USOD methods, they mostly generate pseudo labels by fusing several hand-crafted detectorsâ€™ results. On top of that, a Fully Convolutional Network (FCN) will be trained to detect salient regions separately. While the existing USOD methods have achieved some progress, there are still challenges for them towards satisfactory performance on the complex scene, including 1) poor object wholeness owing to neglecting the hierarchy of those salient regions; 2) unsatisfactory pseudo labels causing by unprimitive fusion of hand-crafted results. To address these issues, in this paper, we introduce the property of part-whole relations endowed by a Belief Capsule Network (BCNet) for deep USOD,which is achieved by a multi-stream capsule routing strategy with a belief score for each stream within the CapsNets architecture. To train BCNet well, we generate high-quality pseudo labels from multiple hand-crafted detectors by developing a consistency-aware fusion strategy. Concretely, a weeding out criterion is first defined to filter out unreliable training samples based on the inter-method consistency among four hand-crafted saliency maps. In the following, a dynamic fusion mechanism is designed to generate high-quality pseudo labels from the remaining samples for BCNet training. Experiments on five public datasets illustrate the superiority of the proposed method. Codes have been released on: https://github.com/Mirlongue/Deep-Unsupervised-Part-Whole-Relational-Visual Saliency.

## Seamless Detection: Unifying Salient Object Detection and Camouflaged Object Detection

### abstract

AchievingjointlearningofSalientObjectDetection(SOD)andCamouflagedObjectDetection(COD) is extremely challenging due to their distinct object characteristics, i.e., saliency and camouflage. The only preliminary research treats them as two contradictory tasks, training models on large scale labeled data alternately for each task and assessing them independently. However, such task specific mechanisms fail to meet real-world demands for addressing unknown tasks effectively. To address this issue, in this paper, we pioneer a task-agnostic framework to unify SOD and COD. To this end, inspired by the agreeable nature of binary segmentation for SOD and COD, we propose a Contrastive Distillation Paradigm (CDP) to distil the foreground from the background, facilitating the identification of salient and camouflaged objects amidst their surroundings. To probe into the contribution of our CDP, we design a simple yet effective contextual decoder involving the interval layer and global context, which achieves an inference speed of 67 fps. Besides the supervised setting, our CDPcanbeseamlesslyintegratedintounsupervisedsettings, eliminating the reliance on extensive humanannotations. Experiments on public SOD and CODdatasets demonstrate the superiority of our proposed framework in both supervised and unsupervised settings, compared with existing state-of the-art approaches. Code is available on https://github.com/liuyi1989/Seamless-Detection.